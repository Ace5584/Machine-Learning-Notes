{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "single-file.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dsk2-RpdukIF"
      },
      "source": [
        "## Titanic File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VbpT3gzbs-bE"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow import keras\r\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "21h8vXCgtH2U",
        "outputId": "971f3d09-44f0-417e-eec8-9702801f2b93"
      },
      "source": [
        "titanic_file_path = tf.keras.utils.get_file(\"train.csv\",  \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tf-datasets/titanic/train.csv\n",
            "32768/30874 [===============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROvyafR7tRIN",
        "outputId": "82deb72a-ec4b-47a0-9394-96af2f7cda2e"
      },
      "source": [
        "titanic_csv_ds = tf.data.experimental.make_csv_dataset(\r\n",
        "    titanic_file_path, batch_size=5,\r\n",
        "    label_name=\"survived\",\r\n",
        "    num_epochs=1,\r\n",
        "    ignore_errors=True\r\n",
        ")\r\n",
        "\r\n",
        "for batch, label in titanic_csv_ds.take(1):\r\n",
        "  for key, value in batch.items():\r\n",
        "    print(f\"{key:20s}: {value}\")\r\n",
        "  print()\r\n",
        "  print(f\"{'label':20s}: {label}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "sex                 : [b'female' b'male' b'male' b'male' b'female']\n",
            "age                 : [34. 24. 34. 58. 24.]\n",
            "n_siblings_spouses  : [1 0 1 0 0]\n",
            "parch               : [1 0 1 0 0]\n",
            "fare                : [32.5     7.8958 14.4    29.7    83.1583]\n",
            "class               : [b'Second' b'Third' b'Third' b'First' b'First']\n",
            "deck                : [b'unknown' b'unknown' b'unknown' b'B' b'C']\n",
            "embark_town         : [b'Southampton' b'Southampton' b'Southampton' b'Cherbourg' b'Cherbourg']\n",
            "alone               : [b'n' b'y' b'n' b'y' b'y']\n",
            "\n",
            "label               : [1 0 0 0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3p_FAnBuYnG"
      },
      "source": [
        "## Decompressing on the Fly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7QUyts8ua0T",
        "outputId": "9f435725-fd47-4cf7-e743-708b267c0792"
      },
      "source": [
        "traffic_volume_csv_gz = tf.keras.utils.get_file(\r\n",
        "    'Metro_Interstate_Traffic_Volume.csv.gz', \r\n",
        "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz\",\r\n",
        "    cache_dir='.', cache_subdir='traffic')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00492/Metro_Interstate_Traffic_Volume.csv.gz\n",
            "409600/405373 [==============================] - 0s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ortB4pKjuqW7",
        "outputId": "caeaedb8-bd9d-4329-d24f-1ee82842882d"
      },
      "source": [
        "traffic_volume_csv_gz_ds = tf.data.experimental.make_csv_dataset(\r\n",
        "    traffic_volume_csv_gz, \r\n",
        "    batch_size=256,\r\n",
        "    label_name='traffic_volume',\r\n",
        "    num_epochs=1,\r\n",
        "    compression_type=\"GZIP\"\r\n",
        ")\r\n",
        "\r\n",
        "for batch, label in traffic_volume_csv_gz_ds.take(1):\r\n",
        "  for key, value in batch.items():\r\n",
        "    print(f\"{key:20s}: {value[:5]}\")\r\n",
        "  print()\r\n",
        "  print(f\"{'label':20s}: {label[:5]}\")\r\n",
        " "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "holiday             : [b'None' b'None' b'None' b'None' b'None']\n",
            "temp                : [278.45 258.79 295.52 268.31 288.93]\n",
            "rain_1h             : [0.   0.   7.54 0.   1.68]\n",
            "snow_1h             : [0. 0. 0. 0. 0.]\n",
            "clouds_all          : [90 63 76 90 44]\n",
            "weather_main        : [b'Clouds' b'Clouds' b'Thunderstorm' b'Haze' b'Rain']\n",
            "weather_description : [b'overcast clouds' b'broken clouds' b'proximity thunderstorm' b'haze'\n",
            " b'moderate rain']\n",
            "date_time           : [b'2012-10-07 13:00:00' b'2013-01-03 05:00:00' b'2013-06-22 03:00:00'\n",
            " b'2013-02-26 02:00:00' b'2013-10-12 00:00:00']\n",
            "\n",
            "label               : [6640 2383  380  229 1297]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}